2021 3강

softmax 함수는, e의 지수에 스코어를 취해 값을 양수로 만들고, 그 합들로 나누어 정규화 하는 것을 의미합니다.
실제로 밑을 e로 하고 지수를 3.2로 계산한다면 대략 24.5가 나오게 되는데, 같은 방법으로 각 클래스별 예측 스코어를 모두 계산해서 모두 0 이상의 양수 값으로 만들어줍니다.
여기에 앞에서 구한 모든 클래스에 해당하는 값들을 더한 값으로 나누어 정규화하면, 예측한 클래스에 대해 0과 1 사이의 확률값 처럼 표현할 수 있습니다. (스코어로 만들어진 값이며, 해당 클래스에 대한 실제"정답 확률"을 의미하지 않기 때문에 실제로 확률은 아니며, 확률처럼 보이고 싶은 값입니다.)
이렇게 구한 확률값에 로그를 취한 뒤 음수를 곱해주면 해당 클래스에 대한 로스값을 구할 수 있게 됩니다.

로스는 예측이 얼마나 잘 안됐는지에 대한 정도를 측정합니다. 그래서 절댓값 측면에서 값이 작아지는 것보다 커지는 방향으로 만들어야 하기 때문에 해당 확률값에 log를 취해주게 됩니다. 그러나 0과 1사이의 로그값은 단조증가 함수로 음수값을 갖게되기 때문에, 로그값에 음수를 곱해주어 로스의 의미와 일치하게 만들어주게 됩니다.

softmax loss는 수식이 정말 헷갈리는 개념이기 때문에 잘 정리해두어야 합니다.

첫번째는 softmax loss L_i의 최솟값과 최댓값에 대한 질문입니다.
음수를 곱해준 로그함수를 0과 1 사이 구간에서 생각해본다면 softmax loss의 최댓값은 양의 무한대이며 최솟값은 0이 됩니다.
그러나 유한정밀도로 인해 최댓값과 최솟값에 도달하지는 못한다고 합니다.
두번째는 초기화 시에 모든 s_j가 거의 같다면, C클래스에서의 softmax loss L_i는 어떻게 될지에 관한 질문입니다.
이는 모든 확률값이 1/C로 동일하기 때문에 – log(1/C)로 표현되고, 이는 log( C )와 동일하다고 답할 수 있습니다.
softmax와 svm을 비교하면, 가중치 W와 입력 이미지 x의 행렬곱에 편향을 더해주는건 같지만, SVM의 경우 스코어에 대한 마진 계산으로 힌지로스를 사용하는 반면, softmax에서는 스코어를 바로 사용하지 않는 대신 밑을 e로 하고 지수에 스코어를 취해주어 정규화를 한 확률값을 만들고 거기에 음의 로그값을 취해 로스를 계산하는 방법을 사용합니다.

67p
질문은 만약 정답 클래스의 스코어 값을 두배로 증가하여 20으로 만들면 SVM loss와 softmax loss는 어떻게 될 지입니다.
우선 svm loss의 경우 세개의 케이스에서 각각 클래스와의 차이가 1보다 컸기 때문에, 로스는 변하지 않게 되므로 모두 로스가 0이 됩니다. 
반면에 softmax loss의 경우 해당 클래스에서 차지하는 확률값이 증가하기 때문에 음의 로그를 취하면 두배 해주기 이전보다 로스가 작아지는 것을 예측할 수 있습니다.
이에 따라 svm loss와 softmax loss의 차이점을 확인할 수 있는데, svm loss의 경우 스코어가 마진 이상으로 높아서 로스가 0이 되면 더이성 개선하지 않는 반면 softmax loss의 경우는 로스 값을 더욱 개선할 수 있는 여지가 있습니다.

softmax의 경우 확률이라는 특징을 갖고있지만, 엄밀하게 따지면 확률은 아닙니다.
해당 클래스가 그 정답인지를 맞추고 싶어하려 하지만, 그렇기 때문에 확률을 의미하진 않습니다.